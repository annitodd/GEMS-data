---
title: "Summary Stats"
format:
  html:
    code-fold: true
    df-print: paged
execute:
  warning: false
---

## Description and Overview

This code summarizes the dataset that is a result of this script: 10_mode-choice-cleaning

GitHub: This .Rmd code (possibly qmd if I have to change it with the new update to R Studio) is synced to Github, and when this code is knit, it's also synced to GitHub, as a .md file, which makes the knitted code easy to read (kind of like python code.) Currently in a FORK: <https://github.com/annitodd/GEMS-data/blob/main/gems-mode-choice>

### Latest Updates

####DEC 6: the next step is to go back to the 10\_ code, and add income

another to-do is ALSO use the raw raw data in the folder that I found

## Setup

### libraries

```{r}
library(arrow)
library(tidyverse)
library(readxl)
library(rstudioapi)
library(scales)
library(writexl)
library(sjmisc)
library(fmlogit)
library(tidyverse) # use this last

```

### file path directories

```{r}
# get current root directory of the user's Github repo
root <- getwd() # Saves current WD 
#while ((basename(root) != "GEMS-data")) {
#  root <- dirname(root)
#} # Sets root equal to the location of the Github repo
#source(file.path(root, "paths.R")) # Runs paths.R file found in users Github repo
```

```{r}
data_path <- 'C:/FHWA/For FHWA folks/Mode_choice_estimation/Data'
data_results <- 'C:/FHWA_R2/mode_choice_estimation/data'
```

## Data: open and slim down

### read in big dataset

full merged dataset

```{r}
df_temp <- read_parquet(file.path(data_results, "10-mode-choice-cleaning_output-full-merged.parquet"))
names(df_temp)
```

Recall that they don't equally merge, as we saw before (hhct has more households on file). Anna notes that this is likely because lots of households took the survey but only some of them filled out the trips diary part. \## START HERE AFTER TGIVING

```{r}
df_temp |>
  count(rawdatafrom_trippub_ATB,rawdatafrom_tripct_ATB,rawdatafrom_hhct_ATB)
df_temp <- ungroup(df_temp)
```

The number of **observations** in the dataset (these should give the same answer):

```{r}
df_temp |> summarise(n())
df_temp |> summarise(n_distinct(HOUSEID,PERSONID,TDTRPNUM))
```

number of **people** in the dataset:

```{r}
df_temp |> summarise(n_distinct(HOUSEID,PERSONID))
```

number of **people** in the dataset **with trips** (there are FEWER people with trips than there are total people:

```{r}
df_temp |> 
  filter(rawdatafrom_trippub_ATB==1,rawdatafrom_tripct_ATB==1,rawdatafrom_hhct_ATB==1) |> summarise(n_distinct(HOUSEID,PERSONID))
```

The number of **households** in the dataset:

```{r}
df_temp |> summarise(n_distinct(HOUSEID))
```

The number of **households** in the dataset **with trips**:

```{r}
df_temp |> 
  filter(rawdatafrom_trippub_ATB==1,rawdatafrom_tripct_ATB==1,rawdatafrom_hhct_ATB==1) |> summarise(n_distinct(HOUSEID))
```

```{r}
#| output: asis

df_trips_only <- df_temp |> 
  filter(rawdatafrom_trippub_ATB==1,rawdatafrom_tripct_ATB==1,rawdatafrom_hhct_ATB==1)

df_trips_only |> 
  count(rawdatafrom_trippub_ATB,rawdatafrom_tripct_ATB,rawdatafrom_hhct_ATB)
df_trips_only <- ungroup(df_trips_only)
```

### list of variables and descriptions

```{r}
names(df_trips_only)
```

This is the data dictionary from NHTS:

```{r}
dictionary_v1_2 <- read_csv("dictionary_v1_2.csv") %>% 
  relocate ("Label","Name")
dictionary_v1_2 
```

### make smaller dataset

choose only some vars: keep all of the ones that we made. These all end in "\_ATB"

```{r}
names(df_trips_only)
df_trips_only_small <- df_trips_only |> 
  select(contains("_ATB")) %>% 
  select(-contains("rawdatafrom"))
```

```{r}
names(df_trips_only_small)
```

### Examine:

#### the key vars

```{r}
names(df_trips_only_small)
```

#### Number of distinct in each group

```{r}
summary <- df_trips_only_small |>
  summarise("Number Obs" = n(),
            across(contains("_ATB"),~n_distinct(.x),.names = "{.col} distinct"),
    .groups = "drop")
summary
summary %>% sjmisc::rotate_df(rn="N distinct")
```

##### the distinct values

```{r}
df_trips_only_small %>% 
  distinct(origin_geoXmicrotype_ATB) 
```

```{r}
df_trips_only_small %>% 
  distinct(dest_geoXmicrotype_ATB) 
```

```{r}
df_trips_only_small %>% 
  distinct(hh_geoXmicrotype_ATB) 
```

```{r}
df_trips_only_small %>% 
  distinct(start_time_bin_ATB) 
```

```{r}
df_trips_only_small %>% 
  distinct(trip_purpose_ATB) 
```

#### how many combinatorials

distinct mode X trip purpose

```{r}
summary <- df_trips_only_small |>
  group_by(mode_ATB,trip_purpose_ATB) %>% 
  summarise("Number Obs" = n(),
            across(contains("_ATB"),~n_distinct(.x),.names = "{.col} distinct"),
    .groups = "drop")
summary
summary %>% sjmisc::rotate_df(rn="N distinct")
```

##### distinct origin destination

```{r}
df_trips_only_small %>% 
  count(origin_geoXmicrotype_ATB,dest_geoXmicrotype_ATB) %>% 
  pivot_wider(names_from = dest_geoXmicrotype_ATB,values_from = n)
```

## 12/7 do only micro types not geo types

```{r}

ggplot() +
  geom_count(data=df_trips_only_small,
             mapping=aes(x = origin_microtypeXgeotype_ATB, y = dest_microtypeXgeotype_ATB)) +
  scale_size_area(max_size = 10)

```

## 12/7 do only micro types not geo types

```{r}

ggplot() +
  geom_count(data=df_trips_only_small,
             mapping=aes(x = origin_geoXmicrotype_ATB, y = dest_geoXmicrotype_ATB)) +
  scale_size_area(max_size = 10)

```

```{r}
df_trips_only_small %>% 
  count(origin_geoXmicrotype_ATB,dest_geoXmicrotype_ATB) 
df_trips_only_small %>% 
  count(origin_geoXmicrotype_ATB,dest_geoXmicrotype_ATB) %>% 
  filter(n<100) %>% 
  ggplot( aes(x = n)) +
  geom_histogram(binwidth = 5)
```

```{r}
temp_count <- df_trips_only_small %>% 
  count(origin_geoXmicrotype_ATB,dest_geoXmicrotype_ATB)

ggplot() +
  geom_tile(data=temp_count,
            mapping=aes(x = origin_geoXmicrotype_ATB, 
                        y = dest_geoXmicrotype_ATB,
                        fill = log(n)))
```

```{r}
ggplot() +
  geom_tile(data=temp_count,
            mapping=aes(x = origin_geoXmicrotype_ATB, 
                        y = dest_geoXmicrotype_ATB,
                        fill = log(n))) +
  geom_point(data=temp_count %>% filter(n<8),
            mapping=aes(x = origin_geoXmicrotype_ATB, 
                        y = dest_geoXmicrotype_ATB,
                        size = -n),
            color = "red",
            alpha = .5) 
  
```

```{r}
ggplot(df_trips_only_small, 
       aes(x = origin_geoXmicrotype_ATB, 
           y = dest_geoXmicrotype_ATB,
           color = start_time_bin_ATB,
           shape = trip_purpose_ATB)) +
  geom_count()
```

```{r}
summary <- df_trips_only_small |>
  group_by(origin_geoXmicrotype_ATB) %>% 
  summarise("Number Obs" = n(),
            across(contains("_ATB"),~n_distinct(.x),.names = "{.col} distinct"),
    .groups = "drop")
36*36
summary
summary %>% sjmisc::rotate_df(rn="N distinct")
```

```{r}
summary <- df_trips_only_small |>
  group_by(origin_geoXmicrotype_ATB,dest_geoXmicrotype_ATB) %>% 
  summarise("Number Obs" = n(),
            across(contains("_ATB"),~n_distinct(.x),.names = "{.col} distinct"),
    .groups = "drop")
36*36
summary
summary %>% sjmisc::rotate_df(rn="N distinct")
```

## Data: make wider dataset

make wider dataset to examine further:

then use the smaller to make wider dataset. This is to make the modes go across rather than down

```{r}
df_trips_only_small <- df_trips_only_small |>
  mutate(onePerObs = 1)
```

```{r}
df_trips_only_small_wide <- df_trips_only_small  |>
  group_by(origin_geoXmicrotype_ATB,dest_geoXmicrotype_ATB) %>% 
  summarise("Number Obs" = n(),
            across(contains("_ATB"),~n_distinct(.x),.names = "{.col}"),
    .groups = "drop")
df_trips_only_small_wide
```

#### How many have all 7 modes?

```{r}
df_trips_only_small_wide  %>% 
  pivot_wider(
    names_from = mode_ATB,
    values_from = "Number Obs",
    values_fn = sum
  )
df_trips_only_small_wide

# df_trips_only_small %>% 
#   count(origin_geoXmicrotype_ATB,dest_geoXmicrotype_ATB) %>% 
#   pivot_wider(names_from = dest_geoXmicrotype_ATB,values_from = n)
```

### make county level dataset

```{r}
# df_county_origin <- df_trips_only_small |>
#   group_by(ORIG_ST,ORIG_CNTY) |>
#   summarise(Ntrips = n(),
#             across(contains("mode_ATB"),~n_distinct(.x),.names = "{.col}_Ndis"),
#     .groups = "drop")
# df_county_origin
```

### make microXgeotype level dataset

```{r}
df_microXgeotype <- df_trips_only_small |>
  group_by(origin_geoXmicrotype_ATB,dest_geoXmicrotype_ATB) |>
  summarise(Ntrips = n(),
            across(contains("_ATB"),~n_distinct(.x),.names = "{.col}_Ndis"),
    .groups = "drop")
df_microXgeotype
```

## START HERE DEC 6

### Wide microgeo by mode to get fractions (first get the sum)

```{r}
df_microXgeotype_wide <- df_trips_only_small |>
    count(origin_geoXmicrotype_ATB,dest_geoXmicrotype_ATB,
          trip_purpose_ATB,start_time_bin_ATB,
          mode_ATB) %>% 
   pivot_wider(
    names_from = mode_ATB,
    values_from = n
  )
df_microXgeotype_wide
```

### new dataset

-   county
-   user class (income variation) use xiaodan's
-   trip purpose -- which may or may not. Mandatory versus non-mandatory. Work and School
-   trip distance bin
-   departure time bin

Micro-geotype -- merge this into the dataset, and then use micro type. Break up by micro geo Replace county by microtype. There are 6 microtypes. Use Origin and destination microtypes -- could do that, then there would be

Need to ask Xiaodan to how to merge microtype and geo type IDs to FIPS blocks.

Then Combinatorials of

## Summary stats by coumty

### basic

how many trips in each state

```{r}
summary_table <- df_trips_only |> 
  group_by(ORIG_ST) |>
  summarise(   
    Ntrips = n(),
    .groups = "drop")
summary_table
```

how many trips for each mode

```{r}
summary_table <- df_trips_only |> 
  group_by(mode_ATB) |>
  summarise(   
    Ntrips = n(),
    .groups = "drop") |>
  arrange(-Ntrips)   
summary_table
```

### by county

how many trips in each county

```{r}
# ggplot(df_county_origin,aes(x=Ntrips)) +
#   geom_histogram(binwidth=1) +
#   coord_cartesian(xlim = c(0, 50))
```

### by mode type by

```{r}
# ggplot(df_county_origin,aes(x=mode_Ndis)) +
#   geom_histogram(binwidth=1) 
# #  coord_cartesian(xlim = c(0, 50))
```

### by purpose

```{r}
# df_county_origin <- df_trips_only_small |>
#   group_by(mode_ATB,trip_purpose_ATB) |>
#   summarise(Ntrips = n(),
#             #across(contains("mode_ATB"),~n_distinct(.x),.names = "{.col}_Ndis"),
#     .groups = "drop")
# #df_county_origin
# df_county_origin
```

```         
      across(where(is.numeric),~mean(.x),.names = "{.col}_avg"),
        across(where(is.factor ),~n_distinct(.x),.names = "{.col}_Ndis"),
```

## Make new proportional dataset

### by microXgeotype

```{r}
# ggplot(df_county_origin,aes(x=Ntrips)) +
#   geom_histogram(binwidth=1) +
#   coord_cartesian(xlim = c(0, 50))
```

## fractional split logit

### methodology

The R package is here, I think it's mostly based on the stata package: https://github.com/f1kidd/fmlogit

It looks like there is no need to include information about how the proportions within each observation were obtained, like how many people or whatever were used to make the proportions, see this from the seminal paper:

Papke, Leslie E., and Jeffrey M. Wooldridge. "Econometric Methods for Fractional Response Variables With an Application to 401 (K) Plan Participation Rates." *Journal of Applied Econometrics*, vol. 11, no. 6, 1996, pp. 619--32. *JSTOR*, http://www.jstor.org/stable/2285155. Accessed 29 Nov. 2023.

![](images/fractional_split_woolridge_2.png){width="465"}

This is an [example](https://www.sciencedirect.com/science/article/abs/pii/S0001457517304049) of a fractional split logit using the fraction of crashes in each TAZ as the y var, and the x vars are descriptors of the TAZ also in proportions. Lee J, Yasmin S, Eluru N, Abdel-Aty M, Cai Q. Analysis of crash proportion by vehicle type at traffic analysis zone level: A mixed fractional split multinomial logit modeling approach with spatial effects. Accid Anal Prev. 2018 Feb;111:12-22. doi: 10.1016/j.aap.2017.11.017. Epub 2017 Nov 20. PMID: 29161538.

```{r}

```

## Render to Word

```{bash}
quarto render 20_mode-choice-cleaned-stats.qmd --to docx
```
